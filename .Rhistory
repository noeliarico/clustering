}
datasets <- random_generator()
random_generator <- function(centers, features) {
desired_length <- length(2:8)*5 #
datasets <- vector(mode = "list", length = desired_length)
ies <- rep(2:8, 5)
set.seed(2020)
for (i in 1:length(ies)) {
data <- clusterlab(centers = ies, # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, ies),
# standard deviation of each cluster
sdvec = sample(1:5, ies),
# how many units to push each cluster away from the initial placement
alphas = sample(1:5, ies),
# the number of features for the data
features = i,
seed = 1234)
data <- t(data$synthetic_data)
datasets[[i]] <- data
}
datasets
}
datasets <- random_generator()
sample(100:200, ies)
sample(1:5, ies)
sample(1:5, ies),
random_generator <- function(centers, features) {
desired_length <- length(2:8)*5 #
datasets <- vector(mode = "list", length = desired_length)
ies <- rep(2:8, 5)
set.seed(2020)
for (i in 1:length(ies)) {
data <- clusterlab(centers = i, # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, i),
# standard deviation of each cluster
sdvec = sample(1:5, i),
# how many units to push each cluster away from the initial placement
alphas = sample(1:5, i),
# the number of features for the data
features = i,
seed = 1234)
data <- t(data$synthetic_data)
datasets[[i]] <- data
}
datasets
}
datasets <- random_generator()
for (i in 2:length(ies)) {
data <- clusterlab(centers = i, # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, i),
# standard deviation of each cluster
sdvec = sample(1:5, i),
# how many units to push each cluster away from the initial placement
alphas = sample(1:5, i),
# the number of features for the data
features = i,
seed = 1234)
data <- t(data$synthetic_data)
datasets[[i-1]] <- data
}
random_generator <- function(centers, features) {
desired_length <- length(2:8)*5 #
datasets <- vector(mode = "list", length = desired_length)
ies <- rep(2:8, 5)
set.seed(2020)
for (i in 2:length(ies)) {
data <- clusterlab(centers = i, # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, i),
# standard deviation of each cluster
sdvec = sample(1:5, i),
# how many units to push each cluster away from the initial placement
alphas = sample(1:5, i),
# the number of features for the data
features = i,
seed = 1234)
data <- t(data$synthetic_data)
datasets[[i-1]] <- data
}
datasets
}
datasets <- random_generator()
random_generator <- function(centers, features) {
desired_length <- length(2:8)*5 #
datasets <- vector(mode = "list", length = desired_length)
ies <- rep(2:8, 5)
set.seed(2020)
for (i in 2:length(ies)) {
data <- clusterlab(centers = i, # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, i),
# standard deviation of each cluster
sdvec = sample(1:i, i),
# how many units to push each cluster away from the initial placement
alphas = sample(1:i, i),
# the number of features for the data
features = i,
seed = 1234)
data <- t(data$synthetic_data)
datasets[[i-1]] <- data
}
datasets
}
datasets <- random_generator()
datasets[[35]]
datasets[[34]]
2:length(ies)
random_generator <- function(centers, features) {
desired_length <- length(2:8)*5 #
datasets <- vector(mode = "list", length = desired_length)
ies <- rep(2:8, 5)
set.seed(2020)
for (i in 1:length(ies)) {
data <- clusterlab(centers = ies[i], # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, i),
# standard deviation of each cluster
sdvec = sample(1:i, i),
# how many units to push each cluster away from the initial placement
alphas = sample(1:i, i),
# the number of features for the data
features = i,
seed = 1234)
data <- t(data$synthetic_data)
datasets[[i]] <- data
}
datasets
}
datasets <- random_generator()
ies[i]
random_generator <- function(centers, features) {
desired_length <- length(2:8)*5 #
datasets <- vector(mode = "list", length = desired_length)
ies <- rep(2:8, 5)
set.seed(2020)
for (i in 1:length(ies)) {
data <- clusterlab(centers = ies[i], # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, ies[i]),
# standard deviation of each cluster
sdvec = sample(1:ies[i], ies[i]),
# how many units to push each cluster away from the initial placement
alphas = sample(1:ies[i], ies[i]),
# the number of features for the data
features = ies[i],
seed = 1234)
data <- t(data$synthetic_data)
datasets[[i]] <- data
}
datasets
}
datasets <- random_generator()
datasets[[35]]
ies <- rep(2:8, 5)
dr <- vector(mode = "list", length = 35)
for(i in 1:35) {
resultsd1 <- train_rkmeans(datasets[[i]], ies[i])
dr[[i]] <- resultsd1
}
calculate_errors <- function(resultsd1) {
errorsd1 <- sapply(resultsd1, function(x) x$tot.withinss)
rownames(errorsd1) <- c(paste0("error_d", distances[1:13]))
errorsd1
#colSums(errorsd1)
}
errors <- lapply(dr, calculate_errors)
errors
sorted_errors_by_dataset <- function(y) {
out <- lapply(1:nrow(y), function(x) {
sort(y[x,])
})
names(out) <- distances[1:13]
out
}
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
lapply(sebd, function(x) lapply(x, consensus::ranking))
lapply(dr, function(x) {unlist(lapply(x, function(y) y$iter))})
lapply(datasets, nor)
lapply(datasets, nrow)
datasets[[28]] == datasets[[35]]
talbe(datasets[[28]] == datasets[[35]])
table(datasets[[28]] == datasets[[35]])
35-28
library(clusterlab)
set.seed(1)
random_generator <- function(centers, features) {
desired_length <- length(2:8)*5 #
datasets <- vector(mode = "list", length = desired_length)
ies <- rep(2:8, 5)
seed <- 2020
for (i in 1:length(ies)) {
data <- clusterlab(centers = ies[i], # the number of clusters to simulate
# the number of units of the radius of the circle on which the clusters are generated
r = 2,
# the number of samples in each cluster
numbervec = sample(100:200, ies[i]),
# standard deviation of each cluster
sdvec = sample(1:ies[i], ies[i]),
# how many units to push each cluster away from the initial placement
alphas = sample(1:ies[i], ies[i]),
# the number of features for the data
features = ies[i],
seed = seed)
seed <- seed + 1
data <- t(data$synthetic_data)
datasets[[i]] <- data
}
datasets
}
datasets <- random_generator()
library(tidyverse)
normalize <- function(x){((x-min(x))/(max(x)-min(x)))}
train_rkmeans <- function(points, cen) {
points <- as_tibble(apply(points, 2, normalize))
print(head(points))
pb <- txtProgressBar(min = 0, max = 15)
desired_length <- 14 # 13+2
results <- vector(mode = "list", length = desired_length)
for(i in 1:13) {
setTxtProgressBar(pb, i)
set.seed(2020)
out_rkmeans <- rkmeans(x = points, centers = cen, iter.max = 100, nstart = 1,
trace = FALSE, distances = 1:13, dist = i)
results[[i]] <- out_rkmeans
}
#
# setTxtProgressBar(pb, 14)
# set.seed(2020)
# out_rkmeans <- rkmeans(x = points, centers = cen, iter.max = 100, nstart = 1,
#                        trace = FALSE, distances = 1:13, dist = 101)
# results[[14]] <- out_rkmeans
#
setTxtProgressBar(pb, 15)
set.seed(2020)
out_rkmeans <- rkmeans(x = points, centers = cen, iter.max = 100, nstart = 1,
trace = FALSE, distances = 1:13, dist = 102)
results[[14]] <- out_rkmeans
close(pb)
names(results) <- distances
return(results)
}
results_a1 <- train_rkmeans(a1, 20)
ies <- rep(2:8, 5)
dr <- vector(mode = "list", length = 35)
for(i in 1:35) {
resultsd1 <- train_rkmeans(datasets[[i]], ies[i])
dr[[i]] <- resultsd1
}
calculate_errors <- function(resultsi) {
errorsi <- sapply(resultsi, function(x) x$tot.withinss)
rownames(errorsi) <- c(paste0("error_d", distances[1:13]))
errorsd1
#colSums(errorsd1)
}
errors <- lapply(dr, calculate_errors)
sorted_errors_by_dataset <- function(y) {
out <- lapply(1:nrow(y), function(x) {
sort(y[x,])
})
names(out) <- distances[1:13]
out
}
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
lapply(sebd, function(x) lapply(x, consensus::ranking))
lapply(dr, function(x) {unlist(lapply(x, function(y) y$iter))})
iters <- lapply(dr, function(x) {unlist(lapply(x, function(y) y$iter))})
lapply(iters, sort)
iters <- lapply(dr, function(x) {unlist(lapply(x, function(y) y$iter))})
lapply(iters, sort)
# Create ranking
lapply(sebd, function(x) lapply(x, consensus::ranking))
sorted_errors_by_dataset <- function(y) {
out <- lapply(1:nrow(y), function(x) {
sort(y[x,])
})
names(out) <- distances[1:13]
out
}
sebd <- lapply(errors, sorted_errors_by_dataset)
sebd
calculate_errors <- function(resultsi) {
errorsi <- sapply(resultsi, function(x) x$tot.withinss)
rownames(errorsi) <- c(paste0("errord_", distances[1:13]))
errorsd1
#colSums(errorsd1)
}
errors <- lapply(dr, calculate_errors)
errors
train_rkmeans <- function(points, cen) {
points <- as_tibble(apply(points, 2, normalize))
print(head(points))
pb <- txtProgressBar(min = 0, max = 15)
desired_length <- 14 # 13+2
results <- vector(mode = "list", length = desired_length)
for(i in 1:13) {
setTxtProgressBar(pb, i)
set.seed(2020)
out_rkmeans <- rkmeans(x = points, centers = cen, iter.max = 100, nstart = 1,
trace = FALSE, distances = 1:13, dist = i)
results[[i]] <- out_rkmeans
}
#
# setTxtProgressBar(pb, 14)
# set.seed(2020)
# out_rkmeans <- rkmeans(x = points, centers = cen, iter.max = 100, nstart = 1,
#                        trace = FALSE, distances = 1:13, dist = 101)
# results[[14]] <- out_rkmeans
#
setTxtProgressBar(pb, 15)
set.seed(2020)
out_rkmeans <- rkmeans(x = points, centers = cen, iter.max = 100, nstart = 1,
trace = FALSE, distances = 1:13, dist = 102)
results[[14]] <- out_rkmeans
close(pb)
names(results) <- distances
return(results)
}
ies <- rep(2:8, 5)
dr <- vector(mode = "list", length = 35)
for(i in 1:35) {
resultsd1 <- train_rkmeans(datasets[[i]], ies[i])
dr[[i]] <- resultsd1
}
calculate_errors <- function(resultsi) {
errorsi <- sapply(resultsi, function(x) x$tot.withinss)
rownames(errorsi) <- c(paste0("errord_", distances[1:13]))
errorsd1
#colSums(errorsd1)
}
errors <- lapply(dr, calculate_errors)
errors
names(dr)
dr[[1]]
names(dr[[1]])
calculate_errors <- function(resultsi) {
errorsi <- sapply(resultsi, function(x) x$tot.withinss)
rownames(errorsi) <- c(paste0("errord_", distances[1:13]))
errorssi
#colSums(errorsd1)
}
errors <- lapply(dr, calculate_errors)
calculate_errors <- function(resultsi) {
errorsi <- sapply(resultsi, function(x) x$tot.withinss)
rownames(errorsi) <- c(paste0("errord_", distances[1:13]))
errorsi
#colSums(errorsd1)
}
errors <- lapply(dr, calculate_errors)
errors
sorted_errors_by_dataset <- function(y) {
out <- lapply(1:nrow(y), function(x) {
sort(y[x,])
})
names(out) <- distances[1:13]
out
}
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
lapply(sebd, function(x) lapply(x, consensus::ranking))
# iters
iters <- lapply(dr, function(x) {unlist(lapply(x, function(y) y$iter))})
lapply(iters, sort)
# Create ranking
lapply(sebd, function(x) lapply(x, consensus::ranking))
lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
# Create ranking
rankingsOfErrors <- lapply(sebd, function(x) lapply(x, consensus::ranking))
lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
warnings()
lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
lapply(rankingsOfErrors, profile_of_rankings)
rankingsOfErrors
# Create ranking
(rankingsOfErrors <- lapply(sebd, function(x) lapply(x, consensus::ranking)))<<
rankingsOfErrors <-lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
rankingsOfErrors <-lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
rankingsOfErrors
detach("package:consensus", unload = TRUE)
remove.packages("consensus")
library(devtools)
install_github("noeliarico/consensus")
library(consensus)
install.packages("~/Desktop/Github/consensus_0.1.0.tar.gz", repos = NULL, type = "source")
library(consensus)
source('~/Desktop/Github/consensus/R/as_ranking.R')
source('~/Desktop/Github/consensus/R/profile_of_rankings.R')
lapply(rankingsOfErrors, profile_of_rankings)
source('~/Desktop/Github/consensus/R/ranking.R')
lapply(rankingsOfErrors, profile_of_rankings)
lapply(rankingsOfErrors, profile_of_rankings)
source('~/Desktop/Github/consensus/R/scoring_ranking_rule.R')
lapply(rankingsOfErrors, borda_count)
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
(rankingsOfErrors <- lapply(sebd, function(x) lapply(x, consensus::ranking)))
rankingsOfErrors <- lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
rankingsOfErrors <- lapply(rankingsOfErrors, profile_of_rankings)
lapply(rankingsOfErrors, borda_count)
sebd[[3]]
pairs_comparison <- expand.grid(d1 = distances, d2 = distances) %>% filter(d1 != d2)
pairs_comparison
pairs_comparison <- crossing(d1 = distances, d2 = distances) %>% filter(d1 != d2)
pairs_comparison
pairs_comparison <- combn(distances) %>% filter(d1 != d2)
combn(distances)
combn(distances, length(distances))
length(distances)
combn(distances, 2)
data.frame(t(combn(distances, 2)))
pairs_comparison <- data.frame(t(combn(distances, 2))) %>% filter(X1 != X2)
tibble(t(combn(distances, 2)))
t(combn(distances, 2))
pairs_comparison <- tibble(t(combn(distances, 2)))
colnames(pairs_comparison) <- c("X1", "X2")
pairs_comparison <- pairs_comparison %>% filter(X1 != X2)
pairs_comparison <- tibble(t(combn(distances, 2)))
pairs_comparison
colnames(pairs_comparison)
pairs_comparison <- tibble(t(combn(distances, 2)), .name_repair = ~ c("a", "b"))
t(combn(distances, 2)
)
nrow(t(combn(distances, 2)))
ncol(t(combn(distances, 2)))
pairs_comparison <- tibble(t(combn(distances, 2)), .name_repair = c("a", "b"))
~ c("a", "b")
pairs_comparison <- tibble(t(combn(distances, 2)), .name_repair =  ~ c("x"))
pairs_comparison
pairs_comparison <- tibble(t(combn(distances, 2)), .name_repair =  ~ c(""))
pairs_comparison
pairs_comparison <- tibble(t(combn(distances, 2)), .cols = 2, .name_repair =  ~ c(""))
pairs_comparison <- tibble(t(combn(distances, 2)), .cols = 2, .name_repair =  ~ c("a", "b"))
pairs_comparison
t(combn(distances, 2)
)
pairs_comparison <- as.tibble(t(combn(distances, 2)), .cols = 2, .name_repair =  ~ c("a", "b"))
pairs_comparison <- as_tibble(t(combn(distances, 2)), .cols = 2, .name_repair =  ~ c("a", "b"))
pairs_comparison
pairs_comparison <- as_tibble(t(combn(distances, 2)), .name_repair =  ~ c("a", "b"))
pairs_comparison
pairs_comparison <- as_tibble(t(combn(distances, 2)), .name_repair =  ~ c("d1", "d2"))
colnames(pairs_comparison) <- c("d1", "d2")
pairs_comparison <- pairs_comparison %>% filter(d1 != d2)
pairs_comparison
rankingsOfErrors <- lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
(rankingsOfErrors <- lapply(sebd, function(x) lapply(x, consensus::ranking)))
rankingsOfErrors <- lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
(rankingsOfErrors <- lapply(sebd, function(x) lapply(x, ranking)))
rankingsOfErrors <- lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
rankingsOfErrors
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
(rankingsOfErrors <- lapply(sebd, function(x) lapply(x, ranking)))
rankingsOfErrors <- lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
rankingsOfErrors <- lapply(rankingsOfErrors, profile_of_rankings)
lapply(rankingsOfErrors, borda_count)
sebd <- lapply(errors, sorted_errors_by_dataset)
# Create ranking
(rankingsOfErrors <- lapply(sebd, function(x) lapply(x, ranking)))
rankingsOfErrors <- lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
rankingsOfErrors
sample(1:10, 50, rep = T)
table(sample(1:10, 50, rep = T))
v <- sample(1:10, 50, rep = T)
table(v, v)
table(v, v, v)
errors <- lapply(dr, calculate_errors)
errors
lapply(error, t)
lapply(errors, t)
errors <- lapply(errors, t)
sebd <- lapply(errors, sorted_errors_by_dataset)
sebd
sorted_errors_by_dataset <- function(y) {
out <- lapply(1:nrow(y), function(x) {
sort(y[x,])
})
names(out) <- distances
out
}
sebd <- lapply(errors, sorted_errors_by_dataset)
sebd
# Create ranking
(rankingsOfErrors <- lapply(sebd, function(x) lapply(x, ranking)))
rankingsOfErrors <- lapply(rankingsOfErrors, function(x) Reduce(bind_rows, x))
rankingsOfErrors <- lapply(rankingsOfErrors, profile_of_rankings)
lapply(rankingsOfErrors, borda_count)
errors <- lapply(dr, calculate_errors)
errors
t(combn(distances, 2))
